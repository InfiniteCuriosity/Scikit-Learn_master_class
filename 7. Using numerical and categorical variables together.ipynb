{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using numerical and categorical variables together\n",
    "In the previous notebooks, we showed the required preprocessing to apply when dealing with numerical and categorical variables. However, we decoupled the process to treat each type individually. In this notebook, we will show how to combine these preprocessing steps.\n",
    "\n",
    "We will first load the entire adult census dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "adult_census = pd.read_csv(\"/Users/russconte/Adult_Census.csv\")\n",
    "# drop the duplicated column `\"education-num\"` as stated in the first notebook\n",
    "adult_census = adult_census.drop(columns=\"Education-num\")\n",
    "\n",
    "target_name = \"Class\"\n",
    "target = adult_census[target_name]\n",
    "\n",
    "data = adult_census.drop(columns=[target_name])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Selection based on data types\n",
    "\n",
    "We will separate categorical and numerical variables using their data types to identify them, as we saw previously that object corresponds to categorical columns (strings). We make use of make_column_selector helper to select the corresponding columns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# In a more general scenario you should manually introspect the content of your dataframe not to wrongly use make_column_selector.\n",
    "\n",
    "# Dispatch columns to a specific processor\n",
    "\n",
    "In the previous sections, we saw that we need to treat data differently depending on their nature (i.e. numerical or categorical).\n",
    "\n",
    "Scikit-learn provides a ColumnTransformer class which will send specific columns to a specific transformer, making it easy to fit a single predictive model on a dataset that combines both kinds of variables together (heterogeneously typed tabular data).\n",
    "\n",
    "We first define the columns depending on their data type:\n",
    "\n",
    "one-hot encoding will be applied to categorical columns. Besides, we use handle_unknown=\"ignore\" to solve the potential issues due to rare categories.\n",
    "numerical scaling numerical features which will be standardized.\n",
    "Now, we create our ColumnTransfomer by specifying three values: the preprocessor name, the transformer, and the columns. First, let’s create the preprocessors for the numerical and categorical parts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we create the transformer and associate each of these preprocessors with their respective columns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "    ('standard_scaler', numerical_preprocessor, numerical_columns)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A ColumnTransformer does the following:\n",
    "\n",
    "It splits the columns of the original dataset based on the column names or indices provided. We will obtain as many subsets as the number of transformers passed into the ColumnTransformer.\n",
    "It transforms each subsets. A specific transformer is applied to each subset: it will internally call fit_transform or transform. The output of this step is a set of transformed datasets.\n",
    "It then concatenates the transformed datasets into a single dataset.\n",
    "The important thing is that ColumnTransformer is like any other scikit-learn transformer. In particular it can be combined with a classifier in a Pipeline:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('one-hot-encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Workclass', 'Education',\n                                                   'Marital-Status',\n                                                   'Occupation', 'Relationship',\n                                                   'Race', 'Sex',\n                                                   'Native-Country']),\n                                                 ('standard_scaler',\n                                                  StandardScaler(),\n                                                  ['Age', 'fnlwgt',\n                                                   'Capital-gain',\n                                                   'Capital-loss',\n                                                   'Hours-per-week'])])),\n                ('logisticregression', LogisticRegression(max_iter=500))])",
      "text/html": "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;one-hot-encoder&#x27;,\n                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n                                                  [&#x27;Workclass&#x27;, &#x27;Education&#x27;,\n                                                   &#x27;Marital-Status&#x27;,\n                                                   &#x27;Occupation&#x27;, &#x27;Relationship&#x27;,\n                                                   &#x27;Race&#x27;, &#x27;Sex&#x27;,\n                                                   &#x27;Native-Country&#x27;]),\n                                                 (&#x27;standard_scaler&#x27;,\n                                                  StandardScaler(),\n                                                  [&#x27;Age&#x27;, &#x27;fnlwgt&#x27;,\n                                                   &#x27;Capital-gain&#x27;,\n                                                   &#x27;Capital-loss&#x27;,\n                                                   &#x27;Hours-per-week&#x27;])])),\n                (&#x27;logisticregression&#x27;, LogisticRegression(max_iter=500))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;one-hot-encoder&#x27;,\n                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n                                                  [&#x27;Workclass&#x27;, &#x27;Education&#x27;,\n                                                   &#x27;Marital-Status&#x27;,\n                                                   &#x27;Occupation&#x27;, &#x27;Relationship&#x27;,\n                                                   &#x27;Race&#x27;, &#x27;Sex&#x27;,\n                                                   &#x27;Native-Country&#x27;]),\n                                                 (&#x27;standard_scaler&#x27;,\n                                                  StandardScaler(),\n                                                  [&#x27;Age&#x27;, &#x27;fnlwgt&#x27;,\n                                                   &#x27;Capital-gain&#x27;,\n                                                   &#x27;Capital-loss&#x27;,\n                                                   &#x27;Hours-per-week&#x27;])])),\n                (&#x27;logisticregression&#x27;, LogisticRegression(max_iter=500))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;one-hot-encoder&#x27;,\n                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n                                 [&#x27;Workclass&#x27;, &#x27;Education&#x27;, &#x27;Marital-Status&#x27;,\n                                  &#x27;Occupation&#x27;, &#x27;Relationship&#x27;, &#x27;Race&#x27;, &#x27;Sex&#x27;,\n                                  &#x27;Native-Country&#x27;]),\n                                (&#x27;standard_scaler&#x27;, StandardScaler(),\n                                 [&#x27;Age&#x27;, &#x27;fnlwgt&#x27;, &#x27;Capital-gain&#x27;,\n                                  &#x27;Capital-loss&#x27;, &#x27;Hours-per-week&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">one-hot-encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Workclass&#x27;, &#x27;Education&#x27;, &#x27;Marital-Status&#x27;, &#x27;Occupation&#x27;, &#x27;Relationship&#x27;, &#x27;Race&#x27;, &#x27;Sex&#x27;, &#x27;Native-Country&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">standard_scaler</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Age&#x27;, &#x27;fnlwgt&#x27;, &#x27;Capital-gain&#x27;, &#x27;Capital-loss&#x27;, &#x27;Hours-per-week&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500)</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=500))\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The final model is more complex than the previous models but still follows the same API (the same set of methods that can be called by the user):\n",
    "\n",
    "the fit method is called to preprocess the data and then train the classifier of the preprocessed data;\n",
    "the predict method makes predictions on new data;\n",
    "the score method is used to predict on the test data and compare the predictions to the expected test labels to compute the accuracy.\n",
    "Let’s start by splitting our data into train and test sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Be aware that we use train_test_split here for didactic purposes, to show the scikit-learn API. In a real setting one might prefer to use cross-validation to also be able to evaluate the uncertainty of our estimation of the generalization performance of a model, as previously demonstrated. Now we can train the model on the train set:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "_= model.fit(data_train, target_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can train the model on the train set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "_ = model.fit(data_train, target_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can send the raw dataset straight to the pipeline. Indeed, we do not need to make any manual preprocessing (calling the transform or fit_transform methods) as it will be handled when calling the predict method. As an example, we predict on the five first samples from the test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "       Age Workclass  fnlwgt   Education       Marital-Status  \\\n7762    56   Private   33115     HS-grad             Divorced   \n23881   25   Private  112847     HS-grad   Married-civ-spouse   \n30507   43   Private  170525   Bachelors             Divorced   \n28911   32   Private  186788     HS-grad   Married-civ-spouse   \n19484   39   Private  277886   Bachelors   Married-civ-spouse   \n\n              Occupation    Relationship    Race      Sex  Capital-gain  \\\n7762       Other-service       Unmarried   White   Female             0   \n23881   Transport-moving       Own-child   Other     Male             0   \n30507     Prof-specialty   Not-in-family   White   Female         14344   \n28911   Transport-moving         Husband   White     Male             0   \n19484              Sales            Wife   White   Female             0   \n\n       Capital-loss  Hours-per-week  Native-Country  \n7762              0              40   United-States  \n23881             0              40   United-States  \n30507             0              40   United-States  \n28911             0              40   United-States  \n19484             0              30   United-States  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Workclass</th>\n      <th>fnlwgt</th>\n      <th>Education</th>\n      <th>Marital-Status</th>\n      <th>Occupation</th>\n      <th>Relationship</th>\n      <th>Race</th>\n      <th>Sex</th>\n      <th>Capital-gain</th>\n      <th>Capital-loss</th>\n      <th>Hours-per-week</th>\n      <th>Native-Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7762</th>\n      <td>56</td>\n      <td>Private</td>\n      <td>33115</td>\n      <td>HS-grad</td>\n      <td>Divorced</td>\n      <td>Other-service</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>23881</th>\n      <td>25</td>\n      <td>Private</td>\n      <td>112847</td>\n      <td>HS-grad</td>\n      <td>Married-civ-spouse</td>\n      <td>Transport-moving</td>\n      <td>Own-child</td>\n      <td>Other</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>30507</th>\n      <td>43</td>\n      <td>Private</td>\n      <td>170525</td>\n      <td>Bachelors</td>\n      <td>Divorced</td>\n      <td>Prof-specialty</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>14344</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>28911</th>\n      <td>32</td>\n      <td>Private</td>\n      <td>186788</td>\n      <td>HS-grad</td>\n      <td>Married-civ-spouse</td>\n      <td>Transport-moving</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>19484</th>\n      <td>39</td>\n      <td>Private</td>\n      <td>277886</td>\n      <td>Bachelors</td>\n      <td>Married-civ-spouse</td>\n      <td>Sales</td>\n      <td>Wife</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>United-States</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([' <=50K', ' <=50K', ' >50K', ' <=50K', ' >50K'], dtype=object)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data_test)[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's compute the accuracy score on the entire test set:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8583244615510606"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(data_test, target_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of the model with cross-validation\n",
    "\n",
    "As previously stated, a predictive model should be evaluated by cross-validation. Our model is usable with the cross-validation tools of scikit-learn as any other predictors:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "{'fit_time': array([1.14887309, 0.98978686, 1.0173769 , 1.01391792, 1.10054231]),\n 'score_time': array([0.02944303, 0.0290072 , 0.02940702, 0.0290482 , 0.02809978]),\n 'test_score': array([0.85228785, 0.85085474, 0.84930385, 0.85257985, 0.85667486])}"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=5)\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross-validation accuracy is: 0.852 ± 0.002\n"
     ]
    }
   ],
   "source": [
    "scores = cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The compound model has a higher predictive accuracy than the two models that used numerical and categorical variables in isolation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fitting a more powerful model\n",
    "\n",
    "Linear models are nice because they are usually cheap to train, small to deploy, fast to predict and give a good baseline.\n",
    "\n",
    "However, it is often useful to check whether more complex models such as an ensemble of decision trees can lead to higher predictive performance. In this section we will use such a model called gradient-boosting trees and evaluate its generalization performance. More precisely, the scikit-learn model we will use is called HistGradientBoostingClassifier. Note that boosting models will be covered in more detail in a future module.\n",
    "\n",
    "For tree-based models, the handling of numerical and categorical variables is simpler than for linear models:\n",
    "\n",
    "we do <b>not need to scale the numerical features</b>\n",
    "using an <b>ordinal encoding for the categorical variables</b> is fine even if the encoding results in an arbitrary ordering\n",
    "Therefore, for HistGradientBoostingClassifier, the preprocessing pipeline is slightly simpler than the one we saw earlier for the LogisticRegression:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                          unknown_value=-1)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns)],\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've made our model, we can check the performance of the model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 5.79 s, total: 22.2 s\n",
      "Wall time: 5.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = model.fit(data_train, target_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8798624191302924"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(data_test, target_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can observe that we get significantly higher accuracies with the Gradient Boosting model. This is often what we observe whenever the dataset has a large number of samples and limited number of informative features (e.g. less than 1000) with a mix of numerical and categorical variables.\n",
    "\n",
    "This explains why Gradient Boosted Machines are very popular among datascience practitioners who work with tabular data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Repeat the gradient boosting process with the diamonds data set:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "diamonds = pd.read_csv('/Users/russconte/diamonds.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "   carat      cut color clarity  depth  table  price     x     y     z\n0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>carat</th>\n      <th>cut</th>\n      <th>color</th>\n      <th>clarity</th>\n      <th>depth</th>\n      <th>table</th>\n      <th>price</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.23</td>\n      <td>Ideal</td>\n      <td>E</td>\n      <td>SI2</td>\n      <td>61.5</td>\n      <td>55.0</td>\n      <td>326</td>\n      <td>3.95</td>\n      <td>3.98</td>\n      <td>2.43</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.21</td>\n      <td>Premium</td>\n      <td>E</td>\n      <td>SI1</td>\n      <td>59.8</td>\n      <td>61.0</td>\n      <td>326</td>\n      <td>3.89</td>\n      <td>3.84</td>\n      <td>2.31</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.23</td>\n      <td>Good</td>\n      <td>E</td>\n      <td>VS1</td>\n      <td>56.9</td>\n      <td>65.0</td>\n      <td>327</td>\n      <td>4.05</td>\n      <td>4.07</td>\n      <td>2.31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.29</td>\n      <td>Premium</td>\n      <td>I</td>\n      <td>VS2</td>\n      <td>62.4</td>\n      <td>58.0</td>\n      <td>334</td>\n      <td>4.20</td>\n      <td>4.23</td>\n      <td>2.63</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.31</td>\n      <td>Good</td>\n      <td>J</td>\n      <td>SI2</td>\n      <td>63.3</td>\n      <td>58.0</td>\n      <td>335</td>\n      <td>4.34</td>\n      <td>4.35</td>\n      <td>2.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = diamonds[\"cut\"]\n",
    "data = diamonds.drop(columns='cut')\n",
    "diamonds.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                          unknown_value=-1)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns)],\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "{'fit_time': array([35.51382399, 41.32982802, 41.22256899, 33.746171  , 37.90856791]),\n 'score_time': array([0.41495395, 0.48189807, 0.27926397, 0.27796197, 0.29607034]),\n 'test_score': array([0.59139785, 0.38107156, 0.24416018, 0.56349648, 0.67927327])}"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=5)\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross-validation accuracy is: 0.492 ± 0.157\n"
     ]
    }
   ],
   "source": [
    "scores = cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is 0.005 better than the previous version using logistic regression which scored at 0.481 ± 0.013"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we:\n",
    "\n",
    "used a ColumnTransformer to apply different preprocessing for categorical and numerical variables;\n",
    "used a pipeline to chain the ColumnTransformer preprocessing and logistic regression fitting;\n",
    "saw that gradient boosting methods can outperform linear models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}